{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "df840597-64ce-4834-852e-48ced451f69f"
      },
      "source": [
        "<a target=\"_blank\" href=\"https://colab.research.google.com/github/google-ai-edge/mediapipe-samples/blob/main/codelabs/litert_inference/gemma3_1b_tflite.ipynb\">\n",
        "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
        "</a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Install Dependencies"
      ],
      "metadata": {
        "id": "39AMoCOa1ckc"
      }
    },
    {
      "metadata": {
        "id": "VoHxuLPu7s37",
        "outputId": "e82ac2b4-1a38-46ac-f1b5-7ccd3daa860f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "cell_type": "code",
      "source": [
        "! wget -q https://github.com/protocolbuffers/protobuf/releases/download/v3.19.0/protoc-3.19.0-linux-x86_64.zip\n",
        "! unzip -o protoc-3.19.0-linux-x86_64.zip -d /usr/local/"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  protoc-3.19.0-linux-x86_64.zip\n",
            "   creating: /usr/local/include/google/\n",
            "   creating: /usr/local/include/google/protobuf/\n",
            "  inflating: /usr/local/include/google/protobuf/wrappers.proto  \n",
            "  inflating: /usr/local/include/google/protobuf/field_mask.proto  \n",
            "  inflating: /usr/local/include/google/protobuf/api.proto  \n",
            "  inflating: /usr/local/include/google/protobuf/struct.proto  \n",
            "  inflating: /usr/local/include/google/protobuf/descriptor.proto  \n",
            "  inflating: /usr/local/include/google/protobuf/timestamp.proto  \n",
            "   creating: /usr/local/include/google/protobuf/compiler/\n",
            "  inflating: /usr/local/include/google/protobuf/compiler/plugin.proto  \n",
            "  inflating: /usr/local/include/google/protobuf/empty.proto  \n",
            "  inflating: /usr/local/include/google/protobuf/any.proto  \n",
            "  inflating: /usr/local/include/google/protobuf/source_context.proto  \n",
            "  inflating: /usr/local/include/google/protobuf/type.proto  \n",
            "  inflating: /usr/local/include/google/protobuf/duration.proto  \n",
            "  inflating: /usr/local/bin/protoc   \n",
            "  inflating: /usr/local/readme.txt   \n"
          ]
        }
      ],
      "execution_count": 1
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install LiteRT Pipeline"
      ],
      "metadata": {
        "id": "qGAaAKzYK5ei"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/google-ai-edge/ai-edge-apis.git#subdirectory=litert_tools"
      ],
      "metadata": {
        "id": "43tAeO0AZ7zp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Create HuggingFace token with permission access to\n",
        "\n",
        "  - litert-community/Gemma3-1B-IT\n",
        "\n",
        "  This is needed to download the tflite model.\n",
        "\n",
        "- Open Colab Secrets: In your Google Colab notebook, locate the Secrets icon in the left-hand sidebar and click on it.\n",
        "- Add a new secret: Click the \"Add Secret\" button.\n",
        "- Name your secret: Enter \"HF_TOKEN\" for your token in the \"Name\" field.\n",
        "- Paste your token: In the \"Value\" field, paste the actual token you want to store."
      ],
      "metadata": {
        "id": "868qAg3KGNVp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create Pipeline from model file"
      ],
      "metadata": {
        "id": "K5okZCTgYpUd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from litert_tools.pipeline import pipeline\n",
        "runner = pipeline.load(\"litert-community/Gemma3-1B-IT\", \"Gemma3-1B-IT_seq128_q8_ekv1280.task\")"
      ],
      "metadata": {
        "id": "3t47HAG2tvc3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generate text from model"
      ],
      "metadata": {
        "id": "dASKx_JtYXwe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Disclaimer: Model performance demonstrated with the Python API in this notebook is not representative of performance on a local device.\n",
        "pipeline = LiteRTLlmPipeline(interpreter, tokenizer)"
      ],
      "metadata": {
        "id": "AZhlDQWg61AL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"What is the capital of France?\"\n",
        "output = runner.generate(prompt, max_decode_steps=None)"
      ],
      "metadata": {
        "id": "wT9BIiATkjzL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}